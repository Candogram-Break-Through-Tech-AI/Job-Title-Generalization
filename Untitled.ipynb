{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2597957-eb18-44fc-bdbd-04c43e2fe0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c632faf-55e7-49f2-ae98-a3d3b6de8e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RequisitionID</th>\n",
       "      <th>OrigJobTitle</th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>JobDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Licensed Stationary Engineer</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>Licensed Stationary Engineer \\n\\n Froedtert So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000224907</td>\n",
       "      <td>Guidance Navigation and Control (GN&amp;C) Enginee...</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**The Boeing Company**  is in search of a  **L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000331804</td>\n",
       "      <td>Propulsion Engineer - Associate, Mid-Level and...</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000336462</td>\n",
       "      <td>Senior Process Controls Engineer</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000338951</td>\n",
       "      <td>RF/Microwave Engineer (Level 2 or 3)</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RequisitionID                                       OrigJobTitle  \\\n",
       "0           NaN                       Licensed Stationary Engineer   \n",
       "1   00000224907  Guidance Navigation and Control (GN&C) Enginee...   \n",
       "2   00000331804  Propulsion Engineer - Associate, Mid-Level and...   \n",
       "3   00000336462                   Senior Process Controls Engineer   \n",
       "4   00000338951               RF/Microwave Engineer (Level 2 or 3)   \n",
       "\n",
       "               JobTitle                                     JobDescription  \n",
       "0  ENGINEER (all other)  Licensed Stationary Engineer \\n\\n Froedtert So...  \n",
       "1  ENGINEER (all other)  **The Boeing Company**  is in search of a  **L...  \n",
       "2  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  \n",
       "3  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  \n",
       "4  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the csv file\n",
    "df = pd.read_csv('data/Engineer_20230826.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736238b0-f7cd-4d37-ba29-d741668e3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Boeing Company**  is in search of a  **Lead Guidance Navigation and Control (GN&C)**   **Engineer**  to join Boeing in the St. Louis area.  **Boeing Defense and Space Systems**  is growing in the area of aircraft, missile, weapons and autonomous system development and integration.\\n\\nSelected candidates will work a wide range of programs that will provide state of the art  **Guidance Navigation and Control (GN&C)**  performance analysis and simulation solutions to our customers. They will have visibility across the Boeing enterprise into modern  **GN&C**  and simulation development methods.\\n\\n**_Come join the Boeing Team!_**\\n\\n**Position Responsibilities:**\\n\\n+ Define Guidance, Navigation and Control (GN&C), mission or trajectory requirements and ensure requirements traceability and quality from the system level to component level\\n\\n+ Design, analyze and guide others in the development of Guidance, Navigation and Control (GN&C) systems, simulations, models & tools\\n\\n+ Lead trade studies and offer guidance and expertise to identify design solutions that meet program requirements\\n\\n+ Lead design reviews, analyses, simulations and component/system testing to ensure delivery of products that meet or exceed customer requirements and expectations\\n\\n+ Work under minimal supervision.\\n\\n**This position requires the ability to obtain a U.S. Security Clearance for which the U.S. Government requires U.S. Citizenship.**  An interim and/or final U.S. Secret Clearance Post-Start is required.\\n\\n**Basic Qualifications (Required Skills/Experience):**\\n\\n+ Bachelor's, Master's or Doctorate of Science degree from an accredited course of study, in engineering, computer science, mathematics, physics or chemistry\\n\\n+ 8+ years pf experience in Guidance Navigation and Controls (GN&C).(5+ years of experience with a Master's degree or 3+ year of experience with a PhD)\\n\\n+ 4+ years of experience using Matlab/Simulink or MatrixX to model systems\\n\\n**Preferred Qualifications (Desired Skills/Experience):**\\n\\n+ Current active US Security Clearance\\n\\n+ Experience with simulation, system and component modeling\\n\\n+ Experience with implementing algorithms in real time systems\\n\\n+ Hardware in the Loop Simulator (HILS) experience\\n\\n**Relocation:**\\n\\nThis position offers relocation based on candidate eligibility.\\n\\n**Shift Work Statement:**\\n\\nThis position is for 1st shift\\n\\n**Drug Free Workplace:**\\n\\nBoeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies.\\n\\nBoeing is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.  <script id=detrack defer src=https://d2e48ltfsb5exy.cloudfront.net/p/t.js?i=0,1 data-g=2804b9e5ace743608ce4359b01b667031691></script>\n"
     ]
    }
   ],
   "source": [
    "# view the pre-cleansed text snippet\n",
    "print(df['JobDescription'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2168d457-a6d6-4b58-a292-54618fc46fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/46jgs1hd0lvg8pn1fr_jwhrc0000gn/T/ipykernel_85283/2420508730.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  cleaned_text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "# function to clean the text so the html/css/js is removed\n",
    "def clean_html(text):   \n",
    "    # use BeautifulSoup to remove HTML tags\n",
    "    cleaned_text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # replace '\\n', '\\\\n', and other similar escape sequences with actual newlines\n",
    "    # cleaned_text = cleaned_text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # strip leading/trailing whitespace or replace multiple newlines with single ones\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df['JobDescription'] = df['JobDescription'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a941d1eb-5bb6-4b6d-b06a-4889b2dee86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Boeing Company**  is in search of a  **Lead Guidance Navigation and Control (GN&C)**   **Engineer**  to join Boeing in the St. Louis area.  **Boeing Defense and Space Systems**  is growing in the area of aircraft, missile, weapons and autonomous system development and integration.\\n\\nSelected candidates will work a wide range of programs that will provide state of the art  **Guidance Navigation and Control (GN&C)**  performance analysis and simulation solutions to our customers. They will have visibility across the Boeing enterprise into modern  **GN&C**  and simulation development methods.\\n\\n**_Come join the Boeing Team!_**\\n\\n**Position Responsibilities:**\\n\\n+ Define Guidance, Navigation and Control (GN&C), mission or trajectory requirements and ensure requirements traceability and quality from the system level to component level\\n\\n+ Design, analyze and guide others in the development of Guidance, Navigation and Control (GN&C) systems, simulations, models & tools\\n\\n+ Lead trade studies and offer guidance and expertise to identify design solutions that meet program requirements\\n\\n+ Lead design reviews, analyses, simulations and component/system testing to ensure delivery of products that meet or exceed customer requirements and expectations\\n\\n+ Work under minimal supervision.\\n\\n**This position requires the ability to obtain a U.S. Security Clearance for which the U.S. Government requires U.S. Citizenship.**  An interim and/or final U.S. Secret Clearance Post-Start is required.\\n\\n**Basic Qualifications (Required Skills/Experience):**\\n\\n+ Bachelor's, Master's or Doctorate of Science degree from an accredited course of study, in engineering, computer science, mathematics, physics or chemistry\\n\\n+ 8+ years pf experience in Guidance Navigation and Controls (GN&C).(5+ years of experience with a Master's degree or 3+ year of experience with a PhD)\\n\\n+ 4+ years of experience using Matlab/Simulink or MatrixX to model systems\\n\\n**Preferred Qualifications (Desired Skills/Experience):**\\n\\n+ Current active US Security Clearance\\n\\n+ Experience with simulation, system and component modeling\\n\\n+ Experience with implementing algorithms in real time systems\\n\\n+ Hardware in the Loop Simulator (HILS) experience\\n\\n**Relocation:**\\n\\nThis position offers relocation based on candidate eligibility.\\n\\n**Shift Work Statement:**\\n\\nThis position is for 1st shift\\n\\n**Drug Free Workplace:**\\n\\nBoeing is a Drug Free Workplace where post offer applicants and employees are subject to testing for marijuana, cocaine, opioids, amphetamines, PCP, and alcohol when criteria is met as outlined in our policies.\\n\\nBoeing is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.\n"
     ]
    }
   ],
   "source": [
    "# view a snippet of cleansed job description\n",
    "print(df['JobDescription'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e3b867-17f6-4bd6-b928-15c1f6993386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RequisitionID</th>\n",
       "      <th>OrigJobTitle</th>\n",
       "      <th>JobTitle</th>\n",
       "      <th>JobDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Licensed Stationary Engineer</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>Licensed Stationary Engineer \\n\\n Froedtert So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000224907</td>\n",
       "      <td>Guidance Navigation and Control (GN&amp;C) Enginee...</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**The Boeing Company**  is in search of a  **L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000331804</td>\n",
       "      <td>Propulsion Engineer - Associate, Mid-Level and...</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000336462</td>\n",
       "      <td>Senior Process Controls Engineer</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000338951</td>\n",
       "      <td>RF/Microwave Engineer (Level 2 or 3)</td>\n",
       "      <td>ENGINEER (all other)</td>\n",
       "      <td>**Job Description**\\n\\nAt Boeing, we innovate ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RequisitionID                                       OrigJobTitle  \\\n",
       "0           NaN                       Licensed Stationary Engineer   \n",
       "1   00000224907  Guidance Navigation and Control (GN&C) Enginee...   \n",
       "2   00000331804  Propulsion Engineer - Associate, Mid-Level and...   \n",
       "3   00000336462                   Senior Process Controls Engineer   \n",
       "4   00000338951               RF/Microwave Engineer (Level 2 or 3)   \n",
       "\n",
       "               JobTitle                                     JobDescription  \n",
       "0  ENGINEER (all other)  Licensed Stationary Engineer \\n\\n Froedtert So...  \n",
       "1  ENGINEER (all other)  **The Boeing Company**  is in search of a  **L...  \n",
       "2  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  \n",
       "3  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  \n",
       "4  ENGINEER (all other)  **Job Description**\\n\\nAt Boeing, we innovate ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffec2d3d-51b6-4a57-a12e-41343dfa1eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 4)\n"
     ]
    }
   ],
   "source": [
    "# select 500 random job postings\n",
    "df_sampled = df.sample(n=500, random_state=42) # random state is for reproducibility\n",
    "print(df_sampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e8582-204b-490d-9f76-2a3a5c85ea18",
   "metadata": {},
   "source": [
    "The API configuration and data collection section. We only run this once and store the output in a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4853a-d15c-437a-b3a4-eaad63d1f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if first time, uncomment line below and run it\n",
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acd59d4-4212-41e7-a18e-69bcfe880a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from environment variable\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def call_openai_api(prompt): \n",
    "    try: \n",
    "        chat_completion = client.chat.completions.create( \n",
    "            messages=[ { \"role\": \"user\", \"content\": \"follow the format and DON't other weird delimiters.\" + prompt, } ], \n",
    "            model=\"gpt-4o-mini\", # stream = True \n",
    "        )\n",
    "            \n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e821010-bd86-45e5-a7e7-34d59218a9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how can i help you'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_openai_api('Hello. Return \"how can i help you\" if this works.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1533349-1db9-4a97-8c89-636b8a2dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to send each job posting to GPT-4o mini and categorize it\n",
    "def categorize_job_posting(job_posting):\n",
    "    # Build the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Break up the following job description into the following categories: Marketing, Description, Requirements, Legal.\n",
    "    For each category, return an array containing the sentences from the job posting corresponding to the category. Return this in JSON formatting\n",
    "\n",
    "    Job Posting:\n",
    "    \"{job_posting}\"\n",
    "\n",
    "    The format should be as follows:\n",
    "\n",
    "    **Marketing:** [Any sentence that promotes the job or company]\n",
    "    **Description:** [Any sentence that explains the job role, company, or purpose of the role]\n",
    "    **Requirements:** [Any sentence that outlines qualifications or skills needed for the role]\n",
    "    **Legal:** [Any sentence that includes legal or compliance information such as equal opportunity statements, privacy, or legal disclaimers]\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the GPT-4o mini API with the prompt\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  # Use the specific GPT-4o mini model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Follow the format and DON'T use other weird delimiters.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        #max_tokens=1000  # Adjust tokens based on job posting length\n",
    "    )\n",
    "\n",
    "    # Extract the response text\n",
    "    categorized_text = response['choices'][0]['message']['content']\n",
    "    return categorized_text\n",
    "\n",
    "# Process all 500 job postings and store the results\n",
    "df_sampled['Categorized_Posting'] = df_sampled['JobDescription'].apply(categorize_job_posting)\n",
    "\n",
    "# Save the results to a data frame as JASON objects\n",
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02a127-7b8f-4b5b-a09c-ebbf18318f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled['Categorized_Posting'][9666]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c278878-74a5-47bd-b2b3-4aedb1e08fdb",
   "metadata": {},
   "source": [
    "Now that we have our data file of 500 random postings, we can start training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca905d9-343c-4bcc-a6b9-2a9dab73fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      JobDescription  \\\n",
      "0  SURVIVABILITY ENGINEER 2\\n\\nLocation:\\n\\nNewpo...   \n",
      "1  Requisition Number: 15838 \\n\\nRequired Travel:...   \n",
      "2  **Secure our Nation, Ignite your Future**\\n\\n*...   \n",
      "3  Description:\\n\\nWe are looking for an Instrume...   \n",
      "4  RH2 is currently seeking an entry-level Staff ...   \n",
      "\n",
      "                                 Categorized_Posting  \n",
      "0  ```json\\n{\\n  \"Marketing\": [\\n    \"Want to be ...  \n",
      "1  ```json\\n{\\n    \"Marketing\": [\\n        \"HII b...  \n",
      "2  ```json\\n{\\n    \"Marketing\": [\\n        \"Secur...  \n",
      "3  ```json\\n{\\n    \"Marketing\": [\\n        \"At Ac...  \n",
      "4  ```json\\n{\\n  \"Marketing\": [\\n    \"Our team is...  \n"
     ]
    }
   ],
   "source": [
    "# Load the api output CSV file\n",
    "df_output = pd.read_csv('data/output_file.csv')\n",
    "\n",
    "# Check the first few rows to ensure the data looks good\n",
    "print(df_output.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190549dc-b3bd-4605-98f3-77aaacb4013e",
   "metadata": {},
   "source": [
    "Now we will preprocess the text data to prepare it for training. FastText is a shallow neural network model that generates embeddings for words and sentences. It handles the vectorization of text internally (through word embeddings) and doesn’t require TfidfVectorizer or manual feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6db1d25f-3214-4568-91f4-df1f46607bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./candogram/lib/python3.11/site-packages (from scikit-learn) (2.1.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83599d0-bbe3-450d-a603-c199442cf689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['JobDescription', 'Categorized_Posting'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# we realize that we only have two columns, with categorized_posting having a json structure that we need to convert\n",
    "print(df_output.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b84e76-c0cd-43d8-8615-1eb463974c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobDescription</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SURVIVABILITY ENGINEER 2\\n\\nLocation:\\n\\nNewpo...</td>\n",
       "      <td>[With more than 25,000 employees - including t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Requisition Number: 15838 \\n\\nRequired Travel:...</td>\n",
       "      <td>[HII Mission Technologies Division is seeking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>**Secure our Nation, Ignite your Future**\\n\\n*...</td>\n",
       "      <td>[Seeking an experienced Automation &amp; Orchestra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Description:\\n\\nWe are looking for an Instrume...</td>\n",
       "      <td>[We are looking for an Instrumentation and Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RH2 is currently seeking an entry-level Staff ...</td>\n",
       "      <td>[RH2 is currently seeking an entry-level Staff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      JobDescription  \\\n",
       "0  SURVIVABILITY ENGINEER 2\\n\\nLocation:\\n\\nNewpo...   \n",
       "1  Requisition Number: 15838 \\n\\nRequired Travel:...   \n",
       "2  **Secure our Nation, Ignite your Future**\\n\\n*...   \n",
       "3  Description:\\n\\nWe are looking for an Instrume...   \n",
       "4  RH2 is currently seeking an entry-level Staff ...   \n",
       "\n",
       "                                         Description  \n",
       "0  [With more than 25,000 employees - including t...  \n",
       "1  [HII Mission Technologies Division is seeking ...  \n",
       "2  [Seeking an experienced Automation & Orchestra...  \n",
       "3  [We are looking for an Instrumentation and Con...  \n",
       "4  [RH2 is currently seeking an entry-level Staff...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Function to clean and load the JSON-like data\n",
    "def clean_json_string(json_string):\n",
    "    # Remove unwanted formatting (like \"```json\\n\" and trailing \"```\")\n",
    "    cleaned_string = re.sub(r'```json\\n|```', '', json_string)\n",
    "    try:\n",
    "        return json.loads(cleaned_string)  # Convert the cleaned string to a dictionary\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "# Clean the 'Categorized_Posting' column and convert to a dictionary\n",
    "df_output['Categorized_Posting'] = df_output['Categorized_Posting'].apply(clean_json_string)\n",
    "\n",
    "# Extract the 'Description' category from each posting\n",
    "df_output['Description'] = df_output['Categorized_Posting'].apply(lambda x: x.get('Description', []))\n",
    "\n",
    "# Display the first few rows with the extracted 'Description'\n",
    "df_output[['JobDescription', 'Description']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a636c0e8-010f-4f52-882d-30e0b52e1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      JobDescription  \\\n",
      "0  SURVIVABILITY ENGINEER 2\\n\\nLocation:\\n\\nNewpo...   \n",
      "1  Requisition Number: 15838 \\n\\nRequired Travel:...   \n",
      "2  **Secure our Nation, Ignite your Future**\\n\\n*...   \n",
      "3  Description:\\n\\nWe are looking for an Instrume...   \n",
      "4  RH2 is currently seeking an entry-level Staff ...   \n",
      "\n",
      "                                    Description_Text  \n",
      "0  With more than 25,000 employees - including th...  \n",
      "1  HII Mission Technologies Division is seeking a...  \n",
      "2  Seeking an experienced Automation & Orchestrat...  \n",
      "3  We are looking for an Instrumentation and Cont...  \n",
      "4  RH2 is currently seeking an entry-level Staff ...  \n"
     ]
    }
   ],
   "source": [
    "# for joining the description sentences, convert from list format to single string per job posting\n",
    "def join_description_sentences(description_list):\n",
    "    return ' '.join(description_list) if isinstance(description_list, list) else description_list\n",
    "\n",
    "# Apply the function to the 'Description' column\n",
    "df_output['Description_Text'] = df_output['Description'].apply(join_description_sentences)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_output[['JobDescription', 'Description_Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17a4d83-070f-434d-b937-8b4aa8c624ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./candogram/lib/python3.11/site-packages (from fasttext) (65.5.0)\n",
      "Requirement already satisfied: numpy in ./candogram/lib/python3.11/site-packages (from fasttext) (2.1.0)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-macosx_14_0_universal2.whl size=628475 sha256=b2748b7e83ff85bcdf775a818516e47202d4a88e6399f9df32d4f5be52a0f840\n",
      "  Stored in directory: /Users/samiulsaimon/Library/Caches/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9665d2-df70-4d7f-881c-2be32d0f1b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  11132\n",
      "Number of labels: 1\n",
      "Progress: 100.0% words/sec/thread:  508433 lr:  0.000000 avg.loss:  0.000000 ETA:   0h 0m 0s100.1% words/sec/thread:  508548 lr: -0.000607 avg.loss:  0.000000 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# prepare data for fasttext training, needs a specific format\n",
    "import fasttext\n",
    "\n",
    "def prepare_fasttext_data(df, text_column, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for text in df[text_column]:\n",
    "            f.write(f'__label__description\\t{text}\\n')\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Prepare the training data\n",
    "prepare_fasttext_data(df_output, 'Description_Text', 'data/fasttext_train.txt')\n",
    "\n",
    "# Train the FastText model\n",
    "model = fasttext.train_supervised(input='data/fasttext_train.txt', epoch=25, lr=1.0, wordNgrams=2)\n",
    "\n",
    "# Save the model\n",
    "model.save_model(\"data/job_description_classifier.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74febb5f-b376-4164-8d13-85bc42bd0d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./candogram/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./candogram/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./candogram/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./candogram/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./candogram/lib/python3.11/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601628a0-1ec8-4254-9343-aa50f2cfd0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samiulsaimon/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/samiulsaimon/nltk_data', '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/nltk_data', '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/share/nltk_data', '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/samiulsaimon/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='/Users/samiulsaimon/nltk_data')\n",
    "nltk.data.path.append('/Users/samiulsaimon/nltk_data')\n",
    "# List all NLTK data paths where resources are stored\n",
    "print(nltk.data.path)\n",
    "nltk.data.clear_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bf5016-4fa0-4c46-98ff-5d09a700263e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/samiulsaimon/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/share/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/samiulsaimon/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHere is a sentence. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms check if sentence tokenization works.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/samiulsaimon/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/share/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/samiulsaimon/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Here is a sentence. Let's check if sentence tokenization works.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e905e8-d756-4ce1-aa6d-d379b5d96e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/samiulsaimon/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/share/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/samiulsaimon/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/job_description_classifier.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Extract job descriptions\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted_Description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_job_descriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJobDescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Display a few examples\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted_Description\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m, in \u001b[0;36mextract_job_descriptions\u001b[0;34m(df, model, text_column)\u001b[0m\n\u001b[1;32m      5\u001b[0m extracted_descriptions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[text_column]:\n\u001b[0;32m----> 7\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     description_sentences \u001b[38;5;241m=\u001b[39m [sent \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(sent)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__label__description\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     extracted_descriptions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(description_sentences))\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Downloads/Breakthrough Tech/Candogram/candogram/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/samiulsaimon/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/share/nltk_data'\n    - '/Users/samiulsaimon/Downloads/Breakthrough Tech/Candogram/candogram/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/samiulsaimon/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# extract job postings using fasttext model\n",
    "\n",
    "def extract_job_descriptions(df, model, text_column):\n",
    "    extracted_descriptions = []\n",
    "    for text in df[text_column]:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        description_sentences = [sent for sent in sentences if model.predict(sent)[0][0] == '__label__description']\n",
    "        extracted_descriptions.append(' '.join(description_sentences))\n",
    "    return extracted_descriptions\n",
    "\n",
    "# Load the full dataset\n",
    "full_df = pd.read_csv('data/Engineer_20230826.csv')\n",
    "\n",
    "# Load the trained model\n",
    "model = fasttext.load_model(\"data/job_description_classifier.bin\")\n",
    "\n",
    "# Extract job descriptions\n",
    "full_df['Extracted_Description'] = extract_job_descriptions(full_df, model, 'JobDescription')\n",
    "\n",
    "# Display a few examples\n",
    "print(full_df[['JobDescription', 'Extracted_Description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ba745-6199-4884-b501-d49ae16b2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fasttext embeddings\n",
    "def generate_fasttext_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        embedding = model.get_sentence_vector(text)\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = generate_fasttext_embeddings(full_df['Extracted_Description'], model)\n",
    "\n",
    "# Add embeddings to the dataframe (as a list, since pandas doesn't handle numpy arrays well in cells)\n",
    "full_df['Embeddings'] = embeddings.tolist()\n",
    "\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(\"First embedding:\", embeddings[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "candogram",
   "language": "python",
   "name": "candogram"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
